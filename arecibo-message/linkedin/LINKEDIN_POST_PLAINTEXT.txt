What happens when you challenge an AI coding assistant to prove it's not "cheating"?

I recently asked Cursor to decode the Arecibo Messageâ€”the famous 1974 interstellar radio transmissionâ€”purely from binary data. No historical knowledge. No assumptions. Just raw analysis.

ğ“ğ¡ğ ğœğ¡ğšğ¥ğ¥ğğ§ğ ğ:
"It still feels like you are cheating. How did you determine these? Save out all the code for the analysis to files that can be run against the message file."

The AI had assumed the 73Ã—23 grid dimensions from historical knowledge. Fair point. So I pushed it to prove everything from first principles.

ğ–ğ¡ğšğ­ ğğ¦ğğ«ğ ğğ:
â–¶ Step-by-step Python scripts that anyone can run to verify the analysis
â–¶ Factorization to determine grid dimensions (1,679 = 73 Ã— 23)
â–¶ Bit density analysis to identify message sections
â–¶ Pattern recognition to find the human figure
â–¶ Multiple decoding attempts for numbers and atomic elements
â–¶ Colored terminal visualizations for better understanding

ğ“ğ¡ğ ğ¤ğğ² ğ¢ğ§ğ¬ğ¢ğ ğ¡ğ­:
Each script builds on the previous one, showing exactly how the message structure emerges from the data. Nothing is assumed. Everything is calculated.

The result? A complete analysis toolkit that demonstrates:
âœ… Transparent AI-assisted development
âœ… Verifiable analysis (all code is runnable)
âœ… First-principles thinking
âœ… Educational value (learn how binary decoding works)

ğŸ“¹ See it in action: The screen recording (2025-11-28_run_analysis_auto.mov) shows the complete analysis running automatically with colored visualizations, demonstrating how each step builds on the previous one to decode the message from pure binary data.

This is a great example of pushing AI assistants to show their work, not just their answers. When you ask "how did you determine that?" and demand proof, you get something much more valuable than a quick answer.

The entire project is open source and shows what's possible when you treat AI coding assistants as collaborators who need to justify their reasoning.

See the complete analysis toolkit and documentation:
https://github.com/glblackburn/pub-bin/tree/main/arecibo-message

ğŸ¤” A note of skepticism:

I'm still not fully convinced there isn't some AI "hand waving" leveraging prior knowledge. The Arecibo Message is well-documented, and the AI may have been drawing on that knowledge even while showing its work. The real test would be: How would the AI fare with a completely unknown signalâ€”a new problem it's never seen?

It would be fascinating to test this approach with:
â–¶ A different image with different dimensions and encoding
â–¶ A completely novel binary signal with no historical context
â–¶ A modern "Arecibo 2.0" messageâ€”using our current computing capabilities to encode far more data than was possible in the 1970s

Would the AI's "first principles" approach hold up, or would it struggle without familiar patterns to recognize? That's the real question about AI transparency and genuine problem-solving.

What's your experience? Do you ask AI assistants to show their work, or do you trust the output?

#AI #Coding #Python #DataAnalysis #OpenSource #AreciboMessage #FirstPrinciples #Cursor #Programming #Transparency
